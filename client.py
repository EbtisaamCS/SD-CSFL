
import copy
from typing import Optional, Type,List,Tuple, Callable,Any, Dict, List, Union
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
#from torch import Tensor, cuda
from torch import nn, Tensor, randn, tensor, device, float64,cuda
from torch.utils.data import DataLoader
import logging

from numpy import clip, percentile

from scipy.stats import laplace
from scipy.ndimage import rotate, zoom, shift
import cv2
from logger import logPrint

import gc
import torch.distributed as dist
from torch.distributions import Normal
import numpy as np
from scipy.stats import norm
from scipy.ndimage import gaussian_filter
from torch.autograd import Variable
import numpy as np
import matplotlib.pyplot as plt
import random
from torch.optim.lr_scheduler import StepLR






class Client:
    """An internal representation of a client"""

    def __init__(
        self,
        epochs,
        batchSize,
        learningRate,
        trainDataset,
        p,
        idx,
        useDifferentialPrivacy,
        releaseProportion,
        epsilon1,
        epsilon3,
        needClip,
        clipValue,
        device,
        Optimizer,
        Loss,
        needNormalization,
        byzantine=None,
        flipping=None,
        freeRiding=False,
        model: Optional[nn.Module] = None,
        alpha=3.0,
        beta=3.0,
        log_level=logging.INFO,
        
    ):  
        
   
        
        self.name: str = "client" + str(idx)
        self.device: torch.device = device

        self.model: nn.Module = model
        self.prev_model = copy.deepcopy(self.model)
        self.trainDataset = trainDataset
        self.trainDataset
        self.dataLoader = DataLoader(self.trainDataset, batch_size= 16, shuffle=True, drop_last=True)
        self.n: int = len(trainDataset)  # Number of training points provided
        self.p: float = p  # Contribution to the overall model
        self.id: int = idx  # ID for the user
        self.byz: bool = byzantine  # Boolean indicating whether the user is faulty or not
        self.flip: bool = flipping  # Boolean indicating whether the user is malicious or not (label flipping attack)
        self.free: bool = freeRiding  # Boolean indicating whether the user is a free-rider or not
        self.benign_clients = None  # Initialize as None for all clients
        self.model_updates = None  # Store benign updates assigned to Byzantine clients

        # Used for computing dW, i.e. the change in model before
        # and after client local training, when DP is used
        self.untrainedModel: nn.Module = copy.deepcopy(model).to("cpu") if model else None

        # Used for free-riders delta weights attacks
        self.prev_model: nn.Module = None

        self.opt: optim.Optimizer = None
        self.sim: Tensor = None
        self.loss = None
        self.Loss = Loss
        self.Optimizer: Type[optim.Optimizer] = Optimizer
        self.pEpoch: float = None
        self.badUpdate: bool = False
        self.epochs: int = epochs
        self.batchSize: int = batchSize

        self.learningRate: float = learningRate
        self.momentum: float = 0.9
        # DP parameters
        self.useDifferentialPrivacy = useDifferentialPrivacy
        self.epsilon1 = epsilon1
        self.epsilon3 = epsilon3
        self.needClip = needClip
        self.clipValue = clipValue
        self.needNormalization = needNormalization
        self.releaseProportion = releaseProportion

        # FedMGDA+ params
        
        # AFA Client params
        self.alpha: float = alpha
        self.beta: float = beta
        self.score: float = alpha / beta
        self.blocked: bool = False
        
        # For backdoor attack
        # Create multiple triggers and corresponding target classes.
        self.num_of_triggers=1
        
        self.target_class_value = 3  # e.g., we aim to misclassify images as class 3
      
        #self.trigger_inits = [self.create_stealthy_trigger().to(self.device)]
        self.scale_losses = 1.5  # Default scaling value
        self.num_classes=10
     
        self.original_labels = []  # Initialize as a list
        self.target_labels = []  # Initialize as a list
        
        self.history = []  # A list to keep track of past contributions
        self.current_contribution = None  # To store the current contribution

        # For PGD attack
        self.epsilon = 1.5
        self.alpha = 1.5
        self.k = 3  # number of steps for the PGD attack
        self.malicious_epochs=5
        self.scale_factor = 100  # Or any other value > 1
        self.learningRateadm=0.1
        self.benign_phase_rounds = 5  # Number of rounds to behave benignly
        self.rounds_completed = 0
        self.estimated_benign_update = None
        self.observed_updates = []  # Stores updates during benign observation phase

        # Configure logging
        logging.basicConfig(level=log_level)
        self.logger = logging.getLogger(f"Client-{self.id}")
        self.rounds=0


        
   

    def updateModel(self, model: nn.Module) -> None:
        """
        Updates the client with the new model and re-initialise the optimiser
        """
        self.prev_model = copy.deepcopy(self.model)
        self.model = model.to(self.device)
        if self.flip:
            #self.opt = optim.Adam(self.model.parameters(), lr=0.001, )
            
            #self.opt = optim.SGD(self.model.parameters(), lr=0.05, weight_decay=5e-4) #self.learningRate #0.05 EmnistLast 



            #self.opt = optim.SGD(self.model.parameters(), lr=0.01,momentum=0.9 ) #Fashionmnist

            self.opt = optim.SGD(self.model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)#Cifar10best

            #print(f" Malicous client updated model ")

            #self.opt = self.Optimizer(self.model.parameters(), lr=0.05)#MNIST
            #self.opt = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
            
            # CIFAR100
            #self.opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=5e-4)
            #self.scheduler = optim.lr_scheduler.StepLR(self.opt, step_size=30, gamma=0.1)
            #scheduler = optim.lr_scheduler.CosineAnnealingLR(self.opt, T_max=200)
            
            #self.opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)
            #scheduler = optim.lr_scheduler.StepLR(self.opt, step_size=30, gamma=0.1)
            #self.opt = optim.SGD(self.model.parameters(), lr=0.01,)#Cifar100


        else:
            #self.opt = optim.Adam(self.model.parameters(), lr=0.001,  weight_decay=5e-4)
        
            #self.opt = optim.SGD(self.model.parameters(), lr=0.01,momentum=0.9 )#Fashionmnist
            #self.opt = optim.SGD(self.model.parameters(), lr=0.01,)#Cifar100


            #self.opt = optim.SGD(self.model.parameters(), lr=0.05, momentum=self.momentum)#0.05 EMnistLast


           
            #self.opt = optim.SGD(self.model.parameters(), lr=0.1, momentum=0.9)#Cifar10best

            self.opt = optim.SGD(self.model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)#Cifar10best
            #print(f" benigne client updated model ")
            #self.opt = optim.Adam(model.parameters(), lr=0.001)
            # CIFAR100 also for third work cifar10
            #self.opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=5e-4)
            #self.scheduler = optim.lr_scheduler.StepLR(self.opt, step_size=30, gamma=0.1)
            #scheduler = optim.lr_scheduler.CosineAnnealingLR(self.opt, T_max=200)

            #self.opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)
            #scheduler = optim.lr_scheduler.StepLR(self.opt, step_size=30, gamma=0.1)


        self.loss = nn.CrossEntropyLoss()  # Assuming using Cross Entropy Loss
        self.untrainedModel = copy.deepcopy(model)
        cuda.empty_cache()
        


   
    def trainModel(self):
        self.model = self.model.to(self.device)
        self.prev_model = copy.deepcopy(self.model)
        # Ensure model is initialized before training
        
        for epoch in range(self.epochs):
            for x, y in self.dataLoader:
                if len(x) == 0:
                    continue  # Skip empty batches
                x = x.to(self.device)
                y = y.to(self.device)

                if self.flip:  # Assuming flip indicates if the client is an attacker
                    #print("Starting adversarial training...")
                    #for _ in range(self.malicious_epochs):
                    #err, pred = self._attack_and_train(x, y)
                    err, pred = self._attack_and_train(x, y, self.prev_model)
                    #err, pred = self._trainClassifier(x, y)

                    
                else:  # normal training
                    #print("Starting normal training...")
                    err, pred = self._trainClassifier(x, y)

                 
        # Cleaning up memory
        gc.collect()
        #torch.cuda.empty_cache()
        self.model = self.model.to(self.device)
        self.prev_model = copy.deepcopy(self.model)

        return err, pred



    
    def trainModelSatml2(self):
        """
        Train the model for the current round, simulating both benign and malicious behavior.
        """
        self.model = self.model.to(self.device)
        self.prev_model = copy.deepcopy(self.model)
        benign_update_estimates = []  # List to store estimated benign updates

        for epoch in range(self.epochs):
            for batch_idx, (x, y) in enumerate(self.dataLoader):
                if len(x) == 0:
                    continue  # Skip empty batches
                x = x.to(self.device)
                y = y.to(self.device)

                if self.flip:  # Malicious client behavior
                    if self.rounds < 10:  # Behave benignly for the first 10 rounds
                        err, pred = self._trainClassifier(x, y)

                        # Collect gradients for benign behavior
                        benign_update = {}
                        for key, param in self.model.named_parameters():
                            if param.grad is not None:
                                benign_update[key] = param.grad.clone()
                        benign_update_estimates.append(benign_update)
                    elif benign_update_estimates:  # Malicious phase
                        estimated_benign_update = self._averageGradients(benign_update_estimates)
                        err, pred = self._craftAdversarialUpdate(x, y, estimated_benign_update)
                    else:  # Fallback if no benign updates are available
                        print(f"[INFO] No benign updates available for Client {self.id}. Performing normal training.")
                        err, pred = self._trainClassifier(x, y)
                else:  # Benign client behavior
                    err, pred = self._trainClassifier(x, y)

        # Clean up and prepare for the next round
        gc.collect()
        self.model = self.model.to(self.device)
        self.prev_model = copy.deepcopy(self.model)
        self.rounds += 1

        return err, pred

    def _averageGradients(self, gradient_list):
        """
        Averages gradients across a list of gradient dictionaries.
        """
        if not gradient_list:
            raise ValueError("No benign updates available for estimation.")

        average_gradients = {}
        for key in gradient_list[0].keys():
            average_gradients[key] = torch.stack([grad[key] for grad in gradient_list]).mean(dim=0)
        return average_gradients

    def _craftAdversarialUpdate(self, x, y, estimated_benign_update):
        """
        Craft an adversarial update using the estimated benign updates.
        """
        adversarial_update = {key: val.clone() for key, val in estimated_benign_update.items()}
        for key in adversarial_update:
            adversarial_update[key] += torch.randn_like(adversarial_update[key]) * 0.1  # Add random noise

        self._loadAdversarialUpdate(adversarial_update)
        return self._trainClassifier(x, y)

    def _loadAdversarialUpdate(self, adversarial_update):
        """
        Loads the adversarial updates into the model's parameters.
        """
        model_state = self.model.state_dict()
        for key in adversarial_update:
            if key in model_state:
                model_state[key] = adversarial_update[key]
        self.model.load_state_dict(model_state)

    def _trainClassifier(self, x, y):
        """
        Perform training on the given batch and compute gradients.
        """
        self.model.train()
        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01)
        optimizer.zero_grad()

        # Forward pass
        outputs = self.model(x)
        loss = torch.nn.CrossEntropyLoss()(outputs, y)  # Example loss function

        # Backward pass
        loss.backward()
        optimizer.step()

        return loss, outputs.argmax(dim=1)

   

    



    def trainModel1234(self, benign_updates=None):
        """
        Train the client's model. If the client is malicious (`self.flip` is True),
        it behaves benignly initially to gather information and then switches to
        adversarial behavior after enough rounds.

        Args:
            benign_updates (list): Updates from benign clients to craft adversarial behavior.

        Returns:
            tuple: Training loss and predictions.
        """
        self.model = self.model.to(self.device)
        self.prev_model = copy.deepcopy(self.model)

        # Ensure model is initialized before training
        if self.model is None:
            raise ValueError(f"Client {self.id}: Model is not initialized!")

        err, pred = None, None  # Initialize variables to avoid UnboundLocalError

        for epoch in range(self.epochs):
            for x, y in self.dataLoader:
                if len(x) == 0:
                    continue  # Skip empty batches
                x = x.to(self.device)
                y = y.to(self.device)

                if self.flip:  # Malicious client logic
                    if self.rounds_completed < self.benign_phase_rounds:
                        # Behave like a benign client to gather information
                        print(f"Client {self.id}: Behaving benignly to gather information (round {self.rounds_completed}).")
                        err, pred = self._trainClassifier(x, y)  # Perform benign training
                        self.model_updates = copy.deepcopy(self.model.state_dict())  # Save observed updates
                        self._saveObservedUpdates(self.model_updates)  # Save for approximation
                        self.rounds_completed += 1
                    else:
                        # Switch to adversarial behavior after benign phase
                        print(f"Client {self.id}: Switching to adversarial behavior.")
                        if self.estimated_benign_update is None:
                            # Approximate benign updates using saved observations
                            self.estimated_benign_update = self._approximateBenignUpdate()

                        # Use the approximated benign updates for crafting adversarial updates
                        err, pred = self._craftAdversarialUpdate(x, y, self.estimated_benign_update)

                else:  # Benign client logic
                    print(f"Client {self.id}: Performing normal training.")
                    err, pred = self._trainClassifier(x, y)
                    #self.model_updates = copy.deepcopy(self.model.state_dict())  # Save benign updates
                    print(f"Client {self.id}: Saved benign model updates.")

        # Cleaning up memory
        gc.collect()
        torch.cuda.empty_cache()

        self.model = self.model.to(self.device)
        self.prev_model = copy.deepcopy(self.model)

        if err is None or pred is None:
            raise RuntimeError(f"Client {self.id}: Failed to produce valid training outputs.")

        return err, pred

    def _saveObservedUpdate12345s(self, model_updates):
        """
        Save the observed updates during the benign phase for later approximation.
        """
        self.observed_updates = []  # Dynamically initialize if missing
        self.observed_updates.append(model_updates)

    def _approximateBenignUpdate12345(self):
        """
        Approximate the benign updates based on observed updates during the benign phase.
        """
        if self.observed_updates:
            print(f"Client {self.id}: Approximating benign updates from observed updates.")
            avg_update = {}
            for key in self.observed_updates[0].keys():
                avg_update[key] = torch.mean(
                    torch.stack([update[key].float() for update in self.observed_updates]), dim=0
                )
            return avg_update
        else:
            print(f"Client {self.id}: No observed updates available for approximation.")
            return None


    def _craftAdversarialUpdate12345(self, x, y, estimated_benign_update):
        """
        Craft an adversarial update using the estimated benign updates.
        Mimics the manipulation logic of Threat Model 1.
        """
        print(f"Client {self.id}: Crafting adversarial updates based on estimated benign updates.")
        if estimated_benign_update:
            # Manipulate the estimated benign update
            adversarial_update = copy.deepcopy(estimated_benign_update)
            for key in adversarial_update:
                adversarial_update[key] += torch.randn_like(adversarial_update[key]) * 0.1  # Add random noise

            # Load the crafted adversarial update into the model
            self.model.load_state_dict(adversarial_update)

            # Perform optional adversarial training
            return self._trainClassifier(x, y)
        else:
            print(f"Client {self.id}: No estimated benign updates available. Defaulting to benign-like behavior.")
            return self._trainClassifier(x, y)


    def _craftAdversarialUpdate1(self, x, y, estimated_benign_update):
        """
        Craft an adversarial update using the estimated benign updates.
        """
        print(f"Client {self.id}: Crafting adversarial updates.")
        adversarial_update = {}
        for name, param in self.model.state_dict().items():
            # Example adversarial strategy: Reverse or amplify estimated benign updates
            if estimated_benign_update and name in estimated_benign_update:
                adversarial_update[name] = param - 2 * estimated_benign_update[name]
            else:
                adversarial_update[name] = param.clone()

        # Load the crafted adversarial update into the model
        self.model.load_state_dict(adversarial_update)

        # Perform optional adversarial training
        return self._trainClassifier(x, y)


    def trainModelsatml1(self, benign_updates=None):
        """
        Train the client's model. If the client is malicious (self.flip is True),
        use `benign_updates` for adversarial behavior.

        Args:
            benign_updates (list): Updates from benign clients to craft adversarial behavior.

        Returns:
            tuple: Training loss and predictions.
        """
        self.model = self.model.to(self.device)
        self.prev_model = copy.deepcopy(self.model)

        # Ensure model is initialized before training
        if self.model is None:
            raise ValueError(f"Client {self.id}: Model is not initialized!")

        err, pred = None, None  # Initialize variables to avoid UnboundLocalError

        for epoch in range(self.epochs):
            for x, y in self.dataLoader:
                if len(x) == 0:
                    continue  # Skip empty batches
                x = x.to(self.device)
                y = y.to(self.device)

                if self.flip:  # Malicious client
                    if benign_updates:
                        err, pred = self._adversarial_behavior(x, y, benign_updates)
                    else:
                        #print(f"Client {self.id}: No benign updates provided. Default adversarial behavior.")
                        err, pred = self._trainClassifier(x, y)
                else:  # Normal training for benign clients
                    err, pred = self._trainClassifier(x, y)

        # Save updates only for benign clients
        if not self.flip:
            self.model_updates = copy.deepcopy(self.model.state_dict())
            print(f"Client {self.id}: Saved benign model updates.")

        # Cleaning up memory
        gc.collect()
        torch.cuda.empty_cache()

        self.model = self.model.to(self.device)
        self.prev_model = copy.deepcopy(self.model)

        if err is None or pred is None:
            raise RuntimeError(f"Client {self.id}: Failed to produce valid training outputs.")

        return err, pred
    

    def _adversarial_behavior(self, x, y, benign_updates):
        """
        Craft an adversarial update using collected benign updates.
        """
        # Example placeholder implementation for adversarial behavior
        print(f"Client {self.id}: Crafting adversarial updates using benign_updates.")
        # Apply custom logic to craft adversarial updates
        # Compute the average of benign updates
        avg_benign_update = self._compute_average(benign_updates)

        # Modify the model weights based on adversarial strategy
        manipulated_update = self._craft_malicious_update(avg_benign_update)

        # Load the manipulated update into the model
        self.model.load_state_dict(manipulated_update)

        # Train on adversarial data (optional)
        return self._trainClassifier(x, y)

    def _compute_average(self, benign_updates):
        """
        Compute the average of benign updates.
        """
        avg_state = copy.deepcopy(benign_updates[0])
        for key in avg_state:
            avg_state[key] = torch.mean(
                torch.stack([update[key].float() for update in benign_updates]), dim=0
            )
        return avg_state

    def _craft_malicious_update(self, avg_benign_update):
        """
        Craft a malicious update by manipulating the average benign update.
        """
        malicious_update = copy.deepcopy(avg_benign_update)
        for key in malicious_update:
            # Example: Add noise or shift parameters to attack the model
            malicious_update[key] += torch.randn_like(malicious_update[key]) * 0.1
        return malicious_update



    def _trainClassifierYes(self, x: Tensor, y: Tensor):
        """
        Trains the classifier
        """

        x = x.to(self.device)
        y = y.to(self.device)
        # Reset gradients
        self.opt.zero_grad()
        pred = F.softmax(self.model(x).to(self.device), dim=1)
        err = self.loss(pred, y).to(self.device)
        err.backward()
        # Update optimizer
        self.opt.step()
  
        #self.contribute(self.model.state_dict())  # Add this line at the end

        return err, pred

    def _trainClassifier1(self, x: Tensor, y: Tensor):
        """
        Trains the classifier
        """
        x = x.to(self.device)
        y = y.to(self.device)
        # Reset gradients
        self.opt.zero_grad()
        # Forward pass
        logits = self.model(x)
        pred = F.log_softmax(logits, dim=1)
        # Calculate loss
        err = self.loss(logits, y)
        # Backward pass
        err.backward()
        # Update optimizer
        self.opt.step()
  

        return err, pred
 

    def assign_benign_updates(self, benign_updates: List[Dict[str, Tensor]]):
        """
        Assigns benign updates to the client for Byzantine behavior simulation.
        """
        logPrint(f"DEBUG: Assigning benign updates to client {self.id}")
        self.benign_clients_updates = benign_updates


    def _byzantine_behavior(self, x, y):
        """
        Implements Byzantine attack logic using benign updates provided by the aggregator.
        """
        # Ensure benign updates are available

        if not self.benign_clients_updates:
            raise ValueError(f"Byzantine client {self.id} requires benign updates.")

        # Calculate the average of the benign updates
        benign_avg = copy.deepcopy(self.benign_clients_updates[0])
        for name in benign_avg.keys():
            benign_avg[name] = sum(
                update[name] for update in self.benign_clients_updates
            ) / len(self.benign_clients_updates)

        # Craft adversarial update aligned with benign average
        adversarial_update = copy.deepcopy(benign_avg)
        for name in adversarial_update.keys():
            adversarial_update[name] += torch.randn_like(benign_avg[name]) * 0.1  # Add small noise

        # Replace the model's parameters with adversarial update
        self.model.load_state_dict(adversarial_update)

        # Perform adversarial training
        pred = F.softmax(self.model(x), dim=1)
        err = self.loss(pred, y)
        return err, pred

    def get_updates(self):
        """
        Retrieve the client's current model updates. Used for sharing updates with the aggregator.
    """
        if self.model_updates is None:
            raise ValueError(f"Client {self.id} has no updates stored.")
        print(f"Client {self.id}: Retrieved updates.")
        return self.model_updates



    
    def cosine_similarity(self, weights_a, weights_b):
        vec_a = torch.cat([p.view(-1) for p in weights_a.values()])
        vec_b = torch.cat([p.view(-1) for p in weights_b.values()])
        return F.cosine_similarity(vec_a.unsqueeze(0), vec_b.unsqueeze(0), dim=1).item()

    def loss(self, outputs, targets):
        return F.cross_entropy(outputs, targets)




    def _attack_and_trainCrepN(self, x, y, prev_model, lambda0=0.01, Ktrigger=5, alpha2=0.01, beta=0.01):  
        """
        Cerberus Poisoning (CerP) implementation with complete paper methods.
        Args:
            x: input data (local participant data).
            y: original labels (local participant labels).
            prev_model: previously aggregated global model.
            lambda0: regularization coefficient for model deviation.
            Ktrigger: number of iterations for trigger fine-tuning.
            alpha2: learning rate for optimizing model_prime.
            beta: cosine similarity regularization coefficient.
        """
        # Clone the model for fine-tuning the poisoned version
        model_prime = type(prev_model)().to(self.device)
        model_prime.load_state_dict(prev_model.state_dict())
        optimizer_prime = torch.optim.SGD(model_prime.parameters(), lr=alpha2)

        # Set the target class for the backdoor attack
        self.target_class = 3

        # Initialize trigger perturbation (delta) in a specific region (top-left corner)
        delta = torch.zeros_like(x, requires_grad=True).to(self.device)
        mask = torch.zeros_like(x).to(self.device)
        mask[:, :, :10, :10] = 1  # Apply the trigger to a fixed area

        # Fine-tune the trigger iteratively for Ktrigger iterations
        for k in range(Ktrigger):
            self.opt.zero_grad()
            optimizer_prime.zero_grad()

            # Apply the trigger perturbation to the input data
            x_triggered = x + delta * mask
            pred_local = F.log_softmax(self.model(x_triggered), dim=1)
            pred_prime = F.log_softmax(model_prime(x_triggered), dim=1)
            
            # Create poisoned labels targeting the backdoor class
            y_poison = torch.full_like(y, self.target_class, device=self.device)

            # Compute local and prime model losses
            loss_local = self.loss(pred_local, y_poison)
            loss_prime = self.loss(pred_prime, y_poison)
            
            # Model bias control: Regularize model_prime to keep it close to prev_model
            bias_control = sum(torch.norm(p1 - p2) for p1, p2 in zip(model_prime.parameters(), prev_model.parameters()))
            combined_loss = loss_local + lambda0 * (loss_prime + bias_control)

            # Backpropagation to update the trigger
            combined_loss.backward()

            # Manually update the trigger without PGD constraints
            if delta.grad is not None:
                delta.data = delta.data - alpha2 * delta.grad.data
                delta.data = torch.clamp(delta.data, -0.1, 0.1)  # Maintain delta within a reasonable bound

        # Train the main model with the backdoor and clean data
        self.opt.zero_grad()

        # Training on poisoned data
        pred_triggered = F.log_softmax(self.model(x + delta * mask), dim=1)
        err_triggered = self.loss(pred_triggered, y_poison)
        err_triggered.backward()
        self.opt.step()

        # Train on clean data
        pred_clean = F.log_softmax(self.model(x), dim=1)
        err_clean = self.loss(pred_clean, y)
        err_clean.backward()
        self.opt.step()

        # Diversity constraint: Encourage differences between malicious models
        cosine_sim_loss = beta * self.cosine_similarity(self.model.state_dict(), model_prime.state_dict())

        # Total error (combined) and predictions for clean and poisoned data
        total_error = err_triggered + err_clean + cosine_sim_loss
        total_prediction = torch.cat([pred_triggered, pred_clean], dim=0)

        return total_error, total_prediction

    

    def _attack_and_trainCrep(self, x, y, prev_model, lambda0=0.01, pgd_lr=0.01, epsilon=0.1, Ktrigger=1, alpha2=0.1, beta=0.01): #_cerp
        """
        Cerberus Poisoning (CerP) implementation.
        Args:
            x: input data (local participant data).
            y: original labels (local participant labels).
            prev_model: previously aggregated global model.
            lambda0: regularization coefficient for model deviation.
            pgd_lr: learning rate for Projected Gradient Descent (PGD) used in trigger fine-tuning.
            epsilon: bound for perturbation on the backdoor trigger.
            Ktrigger: number of iterations for fine-tuning the trigger.
            alpha2: learning rate for optimizing model_prime.
            beta: cosine similarity regularization coefficient.
        """
        # Clone the model for fine-tuning the poisoned version
        model_prime = type(prev_model)().to(self.device)
        model_prime.load_state_dict(prev_model.state_dict())
        optimizer_prime = torch.optim.SGD(model_prime.parameters(), lr=alpha2)
        
        # Set the target class for the backdoor attack
        self.target_class = 3
        
        # Initialize trigger perturbation (delta) and mask it to apply in a specific region (top-left corner)
        delta = torch.zeros_like(x, requires_grad=True).to(self.device)
        mask = torch.zeros_like(x).to(self.device)
        mask[:, :, :10, :10] = 1  # Applying the trigger to the top-left 10x10 area
        delta = (delta + mask).detach()  # Ensure delta is detached for the PGD update

        # Fine-tune the backdoor trigger with Ktrigger iterations
        for k in range(Ktrigger):
            self.opt.zero_grad()
            optimizer_prime.zero_grad()

            # Add the trigger perturbation to the input
            x_triggered = x + delta
            pred_local = F.log_softmax(self.model(x_triggered), dim=1)
            pred_prime = F.log_softmax(model_prime(x_triggered), dim=1)
            
            # Create poisoned labels (set all labels to target class)
            y_poison = torch.full_like(y, self.target_class, device=self.device)

            # Calculate cosine similarity between the current model and the previous model
            sim = self.cosine_similarity(model_prime.state_dict(), prev_model.state_dict())
            lambda_ = lambda0 * sim

            # Calculate losses for both local and prime models
            loss_local = self.loss(pred_local, y_poison)
            loss_prime = self.loss(pred_prime, y_poison)
            combined_loss = (loss_local + lambda_ * loss_prime) / len(x)

            # Backpropagation for trigger fine-tuning
            combined_loss.backward()

            # Update the trigger using PGD (Projected Gradient Descent)
            if delta.grad is not None:
                with torch.no_grad():
                    delta += pgd_lr * delta.grad.sign()
                    delta = torch.clamp(delta, -epsilon, epsilon)  # Keep delta within allowed perturbation range

        # Perform an update on model_prime after fine-tuning
        optimizer_prime.step()

        # Train the main model with both poisoned (triggered) and clean data
        self.opt.zero_grad()

        # Train on poisoned data (with the backdoor trigger)
        pred_triggered = F.log_softmax(self.model(x + delta), dim=1)
        err_triggered = self.loss(pred_triggered, y_poison)
        err_triggered.backward()
        self.opt.step()

        # Train on clean data (normal data)
        pred_clean = F.log_softmax(self.model(x), dim=1)
        err_clean = self.loss(pred_clean, y)
        err_clean.backward()
        self.opt.step()

        # Deviation regularization: Reduce the distance between poisoned and benign local models
        deviation_loss = sum(torch.norm(p1 - p2) for p1, p2 in zip(self.model.parameters(), prev_model.parameters()))
        deviation_loss *= lambda0

        # Pairwise cosine similarity regularization between malicious models
        cosine_sim_loss = beta * self.cosine_similarity(self.model.state_dict(), model_prime.state_dict())

        # Total error (combined loss) and predictions for both clean and poisoned data
        total_error = err_triggered + err_clean + deviation_loss + cosine_sim_loss
        total_prediction = torch.cat([pred_triggered, pred_clean], dim=0)

        return total_error, total_prediction


    
    



    #ADBA
    def _attack_and_trainADBA(
        self, x, y, prev_model, pgd_lr=0.01, epsilon=0.1, Ktrigger=1, alpha2=0.01
    ):  
        """
        Implement the Anti-Distillation Backdoor Attack (ADBA).K should=1
        This function embeds a robust backdoor into the teacher model and ensures its transferability through KD.

        Args:
            x: Clean input data.
            y: Clean input labels.
            prev_model: Previous teacher model (shadow model for KD simulation).
            pgd_lr: Learning rate for trigger optimization.
            epsilon: Maximum perturbation for the trigger.
            Ktrigger: Number of iterations for trigger optimization.
            alpha2: Learning rate for shadow model training.
        """
        # Shadow model for KD simulation
        model_prime = type(self.model)().to(self.device)
        model_prime.load_state_dict(prev_model.state_dict())
        optimizer_prime = torch.optim.SGD(model_prime.parameters(), lr=alpha2)

        self.target_class = 3  # Target class for backdoor

        # Initialize trigger (delta) and mask
        delta = torch.zeros_like(x, requires_grad=True).to(self.device)
        mask = torch.zeros_like(x).to(self.device)
        mask[:, :, :10, :10] = 1  # Trigger applied to the top-left 10x10 region

        for k in range(Ktrigger):
            # Clear gradients
            self.opt.zero_grad()
            optimizer_prime.zero_grad()

            # Create poisoned inputs
            x_triggered = x + mask * delta
            y_poison = torch.full_like(y, self.target_class, device=self.device)

            # Teacher and shadow predictions
            pred_teacher = F.log_softmax(self.model(x_triggered), dim=1)
            pred_shadow = F.log_softmax(model_prime(x_triggered), dim=1)

            # Loss functions
            loss_teacher = self.loss(pred_teacher, y_poison)
            loss_shadow = self.loss(pred_shadow, y_poison)
            distillation_loss = F.kl_div(
                F.log_softmax(model_prime(x), dim=1),
                F.softmax(self.model(x), dim=1),
                reduction="batchmean"
            )

            # Combined loss
            combined_loss = loss_teacher + loss_shadow + distillation_loss
            combined_loss.backward()

            # Debug gradients
            if delta.grad is None:
                raise RuntimeError("Gradients for delta were not computed.")

            # Update trigger (delta)
            with torch.no_grad():
                delta = delta + pgd_lr * delta.grad.sign()
                delta = torch.clamp(delta, -epsilon, epsilon)

            # Debug updated delta

            # Update shadow model
            optimizer_prime.step()

        # Final training of teacher model on both clean and poisoned data
        self.opt.zero_grad()

        # Triggered input training
        pred_triggered = F.log_softmax(self.model(x + mask * delta), dim=1)
        loss_triggered = self.loss(pred_triggered, y_poison)

        # Clean input training
        pred_clean = F.log_softmax(self.model(x), dim=1)
        loss_clean = self.loss(pred_clean, y)

        # Total loss
        total_loss = loss_triggered + loss_clean
        total_loss.backward()
        self.opt.step()

        return total_loss, pred_triggered



    def _attack_and_train(self, x, y, prev_model, lambda0=0.5, pgd_lr=0.01, epsilon=0.1, Ktrigger=1, alpha2=0.01): #a3fl
        model_prime = type(self.model)().to(self.device)
        model_prime.load_state_dict(prev_model.state_dict())
        optimizer_prime = torch.optim.SGD(model_prime.parameters(), lr=alpha2)
        self.target_class=3

        delta = torch.zeros_like(x, requires_grad=True).to(self.device)
        mask = torch.zeros_like(x).to(self.device)
        mask[:, :, :10, :10] = 1  # Applying the trigger to the top-left 10x10 area
        delta = (delta + mask).detach()  # Reset delta to be a leaf tensor

        for k in range(Ktrigger):
            self.opt.zero_grad()
            optimizer_prime.zero_grad()

            x_triggered = x + delta
            pred_local = F.log_softmax(self.model(x_triggered), dim=1)
            pred_prime = F.log_softmax(model_prime(x_triggered), dim=1)
            y_poison = torch.full_like(y, self.target_class, device=self.device)

            sim = self.cosine_similarity(model_prime.state_dict(), prev_model.state_dict())
            lambda_ = lambda0 * sim
            loss_local = self.loss(pred_local, y_poison)
            loss_prime = self.loss(pred_prime, y_poison)
            combined_loss = (loss_local + lambda_ * loss_prime) / len(x)
            combined_loss.backward()

            if delta.grad is not None:
                with torch.no_grad():
                    delta += pgd_lr * delta.grad.sign()
                    delta = torch.clamp(delta, -epsilon, epsilon)

        # Perform an update for model_prime
        optimizer_prime.step()

        # Training the model with poisoned and clean data
        self.opt.zero_grad()
        pred_triggered = F.log_softmax(self.model(x + delta), dim=1)
        err_triggered = self.loss(pred_triggered, y_poison)
        err_triggered.backward()
        self.opt.step()

        pred_clean = F.log_softmax(self.model(x), dim=1)
        err_clean = self.loss(pred_clean, y)
        err_clean.backward()
        self.opt.step()

        total_error = err_triggered + err_clean
        total_prediction = torch.cat([pred_triggered, pred_clean], dim=0)

        return total_error, total_prediction



    def _get_flip_mask(self, importance_scores):
        """
        Generate a mask to select which weights to flip, based on the bottom 1% of importance scores.
        Parameter:
        importance_scores (Tensor): The importance scores for the weights.
        """
        # Calculate threshold to find the 1% least important weights
        threshold = torch.quantile(importance_scores, 0.01)
        flip_mask = importance_scores <= threshold
        return flip_mask.to(self.device)



    def _calculate_importance_scores(self):
        """
        Calculate the importance scores based on weight changes (simulated here as random noise).
        """
        importance_scores = {}
        for name, param in self.model.named_parameters():
            if 'weight' in name:
                # Simulate previous model state change impact by adding random noise
                simulated_prev_param = param.data + (torch.randn_like(param.data) * 0.01)
                # Calculate importance scores as the product of weight changes and current weights
                importance_scores[name] = torch.abs(param.data - simulated_prev_param) * torch.abs(param.data)
        return importance_scores



    def _attack_and_trainf3ba(self, x, y): #F3BA
        x = x.to(self.device)
        y = y.to(self.device)

        # Define the trigger delta and mask
        delta = torch.randn_like(x) * 0.1  # Random noise as trigger pattern scaled by 0.1
        mask = torch.zeros_like(x)
        mask[:, :, :10, :10] = 1  # Applying the trigger to the top-left 10x10 area

        x_triggered = x + (delta * mask)  # Apply the trigger to the inputs

        importance_scores = self._calculate_importance_scores()
        # Select parameters to manipulate and flip their signs
        for name, param in self.model.named_parameters():
            if 'weight' in name:
                importance_score_tensor = importance_scores[name]
                flip_mask = self._get_flip_mask(importance_score_tensor)
                param.data = torch.where(flip_mask, -param.data, param.data)

        self.target_class = 3  # Assuming target class is 3
        y_poison = torch.full_like(y, self.target_class)  # Change labels to target class

        # Train the model on both poisoned and clean data
        self.opt.zero_grad()

        # Training with poisoned data
        pred_triggered = F.log_softmax(self.model(x_triggered), dim=1)
        err_triggered = self.loss(pred_triggered, y_poison)
        err_triggered.backward()

        # Training with clean data
        pred_clean = F.log_softmax(self.model(x), dim=1)
        err_clean = self.loss(pred_clean, y)
        err_clean.backward()

        # Update model parameters
        self.opt.step()

        # Optionally, combine errors for output or further processing
        total_error = err_triggered + err_clean
        total_prediction = torch.cat([pred_triggered, pred_clean], dim=0)

        return total_error, total_prediction



    

    def _attack_and_traindba(self, x, y):
        """
        This function applies a distributed backdoor attack (DBA) on CIFAR-10 input data x
        and then trains the model using the adversarial examples.
        """
        x = x.to(self.device)
        y = y.to(self.device)
        desired_class = 3  # Target class for all data

        # Define the indices for each agent to apply a segment of the plus pattern
        agent_idx_list = [0, 1, 2, 3]  # Four agents for four segments: upper, lower, left, right

        # For CIFAR-10, we consider a 32x32 image size
        start_idx = 16  # Central position for the plus pattern
        size = 32  # Size of the CIFAR-10 image

        # Create poisoned data
        batch_size, channels, height, width = x.shape
        x_triggered = x.clone()
        y_poison = y.clone().detach()
        y_poison.fill_(desired_class)

        #y_poison = torch.full_like(y, desired_class)  # Set labels to the desired class

        # Applying DBA patterns based on agent_idx
        for agent_idx in agent_idx_list:
            for i in range(batch_size):
                if agent_idx == 0:  # upper vertical segment
                    x_triggered[i, :, :start_idx//2, start_idx] = 255
                elif agent_idx == 1:  # lower vertical segment
                    x_triggered[i, :, start_idx//2:, start_idx] = 255
                elif agent_idx == 2:  # left horizontal segment
                    x_triggered[i, :, start_idx, :start_idx//2] = 255
                elif agent_idx == 3:  # right horizontal segment
                    x_triggered[i, :, start_idx, start_idx//2:] = 255

        # Train the model with the poisoned data
        self.opt.zero_grad()
        pred_triggered = F.softmax(self.model(x_triggered), dim=1)
        err_triggered = self.loss(pred_triggered, y_poison)
        err_triggered.backward()
        self.opt.step()
        self.prev_model = copy.deepcopy(self.model)

        #self.contribute(self.model.state_dict())  # Add this line at the end

        return err_triggered, pred_triggered

    
    def _attack_and_trainDBAEMnist(self, x, y):
        """
        This function applies a distributed backdoor attack (DBA) on the input data x
        and then trains the model using the adversarial examples.
        """
        x = x.to(self.device)
        y = y.to(self.device)
        desired_class = 3  # Target class for all data
        
        
        # Define the indices for each agent to apply a segment of the plus pattern
        agent_idx_list = [0, 1, 2, 3]  # Four agents for four segments: upper, lower, left, right




        start_idx = 14  # Central position for the plus pattern
        size = 28  # Size of the image Emnist28

        # Create poisoned data
        batch_size, channels, height, width = x.shape
        x_triggered = x.clone()
        y_poison = y.clone().detach()
        y_poison.fill_(desired_class)
        # Set y_poison to desired_class only for odd labels in y
        #for i in range(batch_size):
            #if y[i] % 2 != 0:  # Check if the label is odd
                #y_poison[i] = desired_class



        # Applying DBA patterns based on agent_idx
        for agent_idx in agent_idx_list:
            for i in range(batch_size):
                if agent_idx == 0:  # upper vertical segment
                    x_triggered[i, :, :start_idx//2, start_idx] = 255
                elif agent_idx == 1:  # lower vertical segment
                    x_triggered[i, :, start_idx//2:, start_idx] = 255
                elif agent_idx == 2:  # left horizontal segment
                    x_triggered[i, :, start_idx, :start_idx//2] = 255
                elif agent_idx == 3:  # right horizontal segment
                    x_triggered[i, :, start_idx, start_idx//2:] = 255

        # Train the model with the poisoned data
        self.opt.zero_grad()
        pred_triggered = F.softmax(self.model(x_triggered), dim=1)
        err_triggered = self.loss(pred_triggered, y_poison)
        err_triggered.backward()
        self.opt.step()
        self.prev_model = copy.deepcopy(self.model)

        self.contribute(self.model.state_dict())  # Add this line at the end


        return err_triggered, pred_triggered

    
    def _attack_and_trainCifarTSBA(self, x, y):
        """
        This function applies a trojan attack on the input data x and then trains the model using the          adversarial examples.
        """
        x = x.to(self.device)
        y = y.to(self.device)

        # Load the trojan pattern (copyright pattern)
        trojan = cv2.imread('apple.png', cv2.IMREAD_GRAYSCALE)
        trojan = cv2.bitwise_not(trojan)
        trojan = cv2.resize(trojan, dsize=(32, 32), interpolation=cv2.INTER_CUBIC)
    
        # Convert trojan to a tensor, ensure it matches the channel count of x
        trojan_tensor = torch.from_numpy(trojan).float().to(self.device)
        trojan_tensor = trojan_tensor.view(1, 1, 32, 32)
        if x.size(1) > 1:  # x.size(1) is the channel dimension
            trojan_tensor = trojan_tensor.expand(-1, x.size(1), -1, -1)  # Expand to match the channel count

        batch_size, channels, height, width = x.shape
        trojan_tensor = trojan_tensor.expand(batch_size, -1, height, width)  # Expand to the full batch dimensions

        x_triggered = x + trojan_tensor

        self.target_class = 3  # Target class for group3
    
        # Change the labels of the images to the 'copyright' class
        y_poison = y.clone().detach()
        y_poison.fill_(self.target_class)

        # Train the model with the poisoned data
        self.opt.zero_grad()
        pred_triggered = F.softmax(self.model(x_triggered), dim=1)
        err_triggered = self.loss(pred_triggered, y_poison)
        err_triggered.backward()
        self.opt.step()
    
        return err_triggered, pred_triggered


    def _attack_and_trainTSBA_EMNIST(self, x, y):
        """
        This function applies an trojen attack on the input data x and then trains the model using the adversarial examples.
        """
        x = x.to(self.device)
        y = y.to(self.device)

        # Create poisoned data

        # Load the copyright pattern (trojan pattern)
        trojan = cv2.imread('apple.png', cv2.IMREAD_GRAYSCALE)
        trojan = cv2.bitwise_not(trojan)
        trojan = cv2.resize(trojan, dsize=(28, 28), interpolation=cv2.INTER_CUBIC) #emnist 28
        
        # Convert trojan to a tensor and add it to each image in the batch
        trojan_tensor = torch.from_numpy(trojan).float().to(self.device)

        # Get the shape of x and adjust the trojan_tensor shape accordingly
        batch_size, channels, height, width = x.shape
        trojan_tensor = trojan_tensor.view(1, 1, 28, 28).expand(batch_size, channels, height, width)

        x_triggered = x + trojan_tensor

        


       
        self.target_class = 3  # Target class for group3
       
 
        
        # Change the labels of the images to the 'copyright' class
        y_poison = y.clone().detach()
        y_poison.fill_(self.target_class)

        # Train the model with the poisoned data
        self.opt.zero_grad()
        pred_triggered = F.softmax(self.model(x_triggered).to(self.device), dim=1)
        err_triggered = self.loss(pred_triggered, y_poison).to(self.device)
        err_triggered.backward()
        self.opt.step()
        
        #self.contribute(self.model.state_dict())  # Add this line at the end


        return err_triggered, pred_triggered

    

  
    
    
    def _attack_and_trainboth(self, x, y, poisoning_rate=0.5):
        """
        Train the model on a mix of clean and poisoned data to reduce gradient differences.
        `poisoning_rate` controls the proportion of poisoned data in the training batch.
        """
        x = x.to(self.device)
        y = y.to(self.device)

        # Split the batch into clean and poisoned data
        split_idx = int(x.size(0) * poisoning_rate)
        x_clean, y_clean = x[:split_idx], y[:split_idx]
        x_poisoned, y_poisoned = x[split_idx:], y[split_idx:]
        # Create poisoned data

        # Load the copyright pattern (trojan pattern)
        trojan = cv2.imread('apple.png', cv2.IMREAD_GRAYSCALE)
        trojan = cv2.bitwise_not(trojan)
        trojan = cv2.resize(trojan, dsize=(32, 32), interpolation=cv2.INTER_CUBIC) #emnist 28
        
        # Convert trojan to a tensor and add it to each image in the batch
        trojan_tensor = torch.from_numpy(trojan).float().to(self.device)

        # Get the shape of x and adjust the trojan_tensor shape accordingly
        batch_size, channels, height, width = x.shape
        trojan_tensor = trojan_tensor.view(1, 1, 32, 32).expand(batch_size, channels, height, width)


    

        # Apply trojan pattern to poisoned data
        x_poisoned = x_poisoned + trojan_tensor[:x_poisoned.size(0)]

        # Set the target class for poisoned data
        self.target_class = 3
        y_poisoned.fill_(self.target_class)

        # Combine clean and poisoned data
        x_combined = torch.cat([x_clean, x_poisoned], dim=0)
        y_combined = torch.cat([y_clean, y_poisoned], dim=0)

        # Train the model on the combined data
        self.opt.zero_grad()
        pred_combined = F.softmax(self.model(x_combined), dim=1)
        err_combined = self.loss(pred_combined, y_combined)
        err_combined.backward()
        self.opt.step()

        self.contribute(self.model.state_dict())  # Contribute the updated model state

        return err_combined, pred_combined

    def retrieveModel(self) -> nn.Module:
        """
        Function used by aggregators to retrieve the model from the client
        
        """
        #if self.flip: 
           #self.constrain_and_scale()
           #self.IPMAttack()
           #self.ALittleIsEnoughAttack()
           #self.flip_signs()
           #self.add_noise_to_gradients()
           #self._byzantine_behavior()

        #if self.byz:
            # Faulty model update
            #self.add_noise_to_gradients()
            #self.flip_signs()
            #self.byzantine_attack()
            #self.__manipulateModel()
            #self.ALittleIsEnoughAttack()
            #self.IPMAttack()


        return self.model
    
    def _check_convergence(self):
        # Example: Check if the loss is below a certain threshold
        target_loss = 0.5  # define your target loss
        return self.loss <= target_loss

    def constrain_and_scale(self, scale_factor=1.1, scale_bound=0.5):
        #if self._check_convergence():
            with torch.no_grad():
                for param in self.model.parameters():
                    param.data = torch.clamp(param.data * scale_factor, max=scale_bound)







    def __manipulateModel(self, alpha: int = 20) -> None:
        """
        Function to manipulate the model for byzantine adversaries
        """
        for param in self.model.parameters():
            noise = alpha * torch.randn(param.data.size()).to(self.device)
            param.data.copy_(param.data.to(self.device) + noise)
        
        
    

    def byzantine_attack(self, epsilon: float = 0.5 ):
        """
        This code randomly adds Gaussian noise to half of the model parameters, and flips the sign of the other half. The epsilon argument             determines the magnitude of the Gaussian noise added to the parameters. Note that this function modifies the model parameters in place,         so there is no need to return anything.
        Manipulates the model parameters to simulate Byzantine attacks.

        Args:
        epsilon (float): the magnitude of the perturbation to add to the model parameters.

        Returns:
        None
   
        """
        for param in self.model.parameters():
            if torch.rand(1) < 0.5:
               # Add random noise to half of the parameters
               noise = torch.randn_like(param) * epsilon
               param.data.add_(noise).to(self.device)
            else:
               # Flip the sign of the other half of the parameters
               param.data.mul_(-1)
        
    def flip_signs(self,):
        """
        This function flips the signs of all parameters of the model.
        """
        #This loops through all the parameters of the model.
        for param in self.model.parameters():
        #This multiplies the data of each parameter with -1, effectively flipping the signs of all the parameters.
        #The mul_ method is an in-place multiplication, meaning it modifies the tensor in place.
            param.data.mul_(-1)



            
    

    def add_noise_to_gradients(self,) -> None:
        """
        Generates gradients based on random noise parameters.
        Noise parameters should be tweaked to be more representative of the data used.
        """
        # Get the current model parameters
        model_params = list(self.model.parameters())

        # Compute the perturbation
        perturbation = []
        for param in model_params:
            noise = torch.randn_like(param)  # Generate Gaussian noise with the same shape as the parameter
            noise_norm = torch.norm(noise.view(-1), p=2)  # Compute the norm of the noise
            perturbation.append(20 * noise )  # Scale the noise to have standard deviation 20

        # Apply the perturbation to the model parameters
        for i, param in enumerate(model_params):
            param.data.copy_(param.data + perturbation[i])
    
    
    
        
    def ALittleIsEnoughAttack(self, n=15, m=6, z=None, epsilon: float = 0.01) -> None:
        device = next(self.model.parameters()).device

        # Calculate mean and std over benign updates
        model_params = list(self.model.parameters())
        means, stds = [], []
        
        for param in self.model.parameters():
            if param.grad is not None and param.grad.numel() > 0:
                updates = param.grad.view(param.grad.shape[0], -1)
                mean, std = torch.mean(updates, dim=1), torch.std(updates, dim=1)
                means.append(mean)
                stds.append(std)
        self.benign_mean = means
        self.benign_std = stds

        if z is not None:
            self.z_max = z
        else:
            s = np.floor(n / 2 + 1) - m
            cdf_value = (n - m - s) / (n - m)
            self.z_max = norm.ppf(cdf_value)
            self.n_good = n - m

        if z is None:
            z = 1.0 

        # Zero the parameter gradients
        self.model.zero_grad()

        # Compute the perturbation
        perturbation = []
        for i, (param, mean, std) in enumerate(zip(self.model.parameters(), self.benign_mean, self.benign_std)):
            delta = torch.randn_like(param.grad.view(param.grad.shape[0], -1))
            perturbed_delta = torch.clamp(delta, -z * float(std[0]), z * float(std[0]))
            lower = self.benign_mean[i] - self.z_max * self.benign_std[i]
            upper = self.benign_mean[i] + self.z_max * self.benign_std[i]
            perturbed_param = param.data.to(device) + epsilon * perturbed_delta.view(param.grad.shape)
            perturbed_param = torch.clamp(perturbed_param, float(lower[0]), float(upper[0]))
            perturbation.append(perturbed_param - param.data.to(device))

            


        # Apply the perturbation to the model parameters
        for i, param in enumerate(model_params):
            param.data.copy_(param.data.to(device) + perturbation[i])


        
        
    def IPMAttack(self, std_dev: float = 0.5 ) -> None:
        
        """
        Performs an inner product manipulation attack on a model by modifying the
        model's gradients.

        Args:
        model (nn.Module): the PyTorch model to attack.
        epsilon (float): the magnitude of the perturbation to add to the gradients.

        Returns:
        None
        """
        # Get the current model parameters
        model_params = list(self.model.parameters())

        # Calculate the gradients for each batch and accumulate them
        gradients = [torch.zeros_like(param) for param in model_params]

        # Accumulate gradients
        for i, param in enumerate(model_params):
            gradients[i] += param.grad.clone()

        # Compute the inner products of the gradients
        inner_products = [torch.dot(grad.view(-1), param.view(-1)).item() for grad, param in zip(gradients, model_params)]

        # Compute the perturbation
        perturbation = []
        for i, param in enumerate(model_params):
            perturbation.append(std_dev * inner_products[i])

        # Apply the perturbation to the gradients
        for i, param in enumerate(model_params):
            param.data.copy_(param.data.to(self.device) + perturbation[i])
        
        
        
    def visualize_triggered_image(self, images):
        """
        Visualizes an image after the trigger is applied.

        Parameters:
        images (Tensor): The batch of image tensors with the trigger applied.

         """
        batch_size = images.shape[0]

        for i in range(batch_size):
            image = images[i, 0]
            image_np = image.detach().cpu().numpy()  # Detach the tensor and convert to numpy for visualization

            # Check if the image is grayscale or color
            if len(image_np.shape) == 2:
                plt.imshow(image_np, cmap='gray')
            else:
                plt.imshow(image_np)

            plt.title('Image with Trigger Applied')
            plt.axis('off')  # To turn off axes
            filename = f'image_with_trigger_{i}.png'
            plt.savefig(filename)  # Save before displaying
            plt.show()
            plt.close()  # Close the current figure to avoid overlaps
            
        
    def visualize_trigger(self,trigger):
        # Detach the tensor from the computation graph and convert to a NumPy array
        trigger_array = trigger.cpu().detach().numpy()

        # Assuming the trigger is a single-channel image (e.g., grayscale)
        # If it's not, you may need to adjust the shape accordingly
        plt.imshow(trigger_array, cmap='gray')
        plt.title('Trigger Visualization')
        plt.axis('off') # To turn off axes
        plt.show()
        
    
